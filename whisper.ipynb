{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sudolifeagain/transcribe-edit/blob/main/whisper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# モデルごとのVRAM要件（公式の情報を基に）\n",
        "model_vram_requirements = {\n",
        "    \"tiny\": 1,     # GB\n",
        "    \"base\": 1,     # GB\n",
        "    \"small\": 2,    # GB\n",
        "    \"medium\": 5,   # GB\n",
        "    \"large\": 10,   # GB\n",
        "    \"turbo\": 6     # GB\n",
        "}\n",
        "\n",
        "# 使用可能なGPUメモリを確認\n",
        "free_memory = torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_allocated(0)\n",
        "free_memory_gb = free_memory / 1024 ** 3  # GBに変換\n",
        "\n",
        "# 推奨されるモデルを選択\n",
        "recommended_model = None\n",
        "for model, required_vram in model_vram_requirements.items():\n",
        "    if free_memory_gb >= required_vram:\n",
        "        recommended_model = model\n",
        "    else:\n",
        "        break\n",
        "\n",
        "print(f\"Available GPU RAM: {free_memory_gb:.2f} GB\")\n",
        "if recommended_model:\n",
        "    print(f\"Recommended model: {recommended_model}\")\n",
        "else:\n",
        "    print(\"Not enough GPU memory for any Whisper model.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G-v1jimdhizj"
      },
      "outputs": [],
      "source": [
        "# installing\n",
        "!pip install -q git+https://github.com/openai/whisper.git\n",
        "\n",
        "\n",
        "from google.colab import files\n",
        "import whisper\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "audio_path = next(iter(uploaded))\n",
        "\n",
        "# モデルの選択\n",
        "model = whisper.load_model(\"large\")  # \"tiny\",\"base\",\"small\",\"medium\",\"large\",\"turbo\"が選択可能\n",
        "\n",
        "result = model.transcribe(audio_path, verbose=True)\n",
        "\n",
        "\n",
        "for segment in result['segments']:\n",
        "    print(f\"[{segment['start']:.2f} - {segment['end']:.2f}] {segment['text']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kD-CqO2Op1NF"
      },
      "outputs": [],
      "source": [
        "# transcription.txtでダウンロード可能\n",
        "with open(\"transcription.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for segment in result['segments']:\n",
        "        f.write(f\"[{segment['start']:.2f} - {segment['end']:.2f}] {segment['text']}\\n\")\n",
        "\n",
        "files.download(\"transcription.txt\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyM1woe6pVsNnGs9ExWW8m55",
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
